{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SynthMorph Affine PyTorch Demo\n",
    "## Purpose\n",
    "Reproduce the original affine components of SynthMorph demo in Torch.\n",
    "- Data generation with affine augmentations\n",
    "- Affine registration model training\n",
    "- Registration (inference) examples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# local code\n",
    "from synthmorph import networks, models, layers, losses, datamodule as dm, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'     # note: only gpu has been tested so far\n",
    "torch.set_default_device(device)\n",
    "# mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynthMorph Affine Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Label (i.e. Segmentation) Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = (256,) * 2\n",
    "num_labels = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shapes.\n",
    "in_shape = (256,) * 2\n",
    "num_dim = len(in_shape)\n",
    "num_label = 4\n",
    "label_map = dm.generate_map(in_shape, num_label, device=device)\n",
    "n = 4\n",
    "affine_args = dict(\n",
    "    translate=(0.05, 0.05),\n",
    "    scale=(0.9, 0.9)\n",
    ")\n",
    "gen_args = dict(\n",
    "    warp_std=0,\n",
    "    warp_res=(8, 16, 32),\n",
    "    zero_background=1,\n",
    "    affine_args=affine_args,\n",
    ")\n",
    "\n",
    "gen = [dm.labels_to_image(label_map, **gen_args) for _ in tqdm(range(n))]\n",
    "gen_images = [g['image'] for g in gen]\n",
    "gen_labels= [g['label'] for g in gen]\n",
    "\n",
    "plot_num = min(n, 4)\n",
    "fig, axes = plt.subplots(1, plot_num, figsize=(plot_num*8, 8))\n",
    "\n",
    "for i in range(plot_num):\n",
    "    image = gen_images[i].squeeze().tolist()\n",
    "    axes[i].imshow(image, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each label of an image\n",
    "ind = 1\n",
    "image = gen_images[ind].squeeze().tolist()\n",
    "labels = gen_labels[ind].squeeze().tolist()\n",
    "plot_num = gen_labels[ind].shape[0] + 1\n",
    "fig, axes = plt.subplots(1, plot_num, figsize=(plot_num*8, 8))\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].axis('off')\n",
    "for c in range(1, plot_num):\n",
    "    ax = axes[c]\n",
    "    l = labels[c - 1]\n",
    "    ax.imshow(l, cmap='gray')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "size=40\n",
    "in_shape = (256,) * 2\n",
    "num_labels = 16\n",
    "affine_args = dict(\n",
    "    translate=(0.05, 0.05),\n",
    "    scale=(0.9, 0.9)\n",
    ")\n",
    "gen_args = dict(\n",
    "    warp_std=0, # no deformable\n",
    "    warp_res=(8, 16, 32),\n",
    "    zero_background=1,\n",
    "    affine_args=affine_args,\n",
    ")\n",
    "\n",
    "train_data = dm.SMShapesDataset(\n",
    "    size=size,\n",
    "    input_size=in_shape,\n",
    "    num_labels=num_labels,\n",
    "    gen_args=gen_args,\n",
    ")\n",
    "dataloader_kwargs = {'num_workers': 8, 'persistent_workers': True,} if device == 'cuda' else {}\n",
    "dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator(device=device),\n",
    "    **dataloader_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can generate the Torch version of the original author's weights from tf2torch.ipynb\n",
    "# State dict weights for the registration model, different from PL checkpoint\n",
    "weights_path = Path(\".\") / 'weights'\n",
    "# reg_weights = weights_path / 'torch' / \"authors.pth\"   # 'None' for no weight loading\n",
    "reg_weights = None\n",
    "# Fresh model\n",
    "in_shape = (256,) * 2\n",
    "enc_nf = [256] * 4\n",
    "dec_nf = [256] * 0\n",
    "add_nf = [256] * 4\n",
    "model = models.SynthMorphAffine(\n",
    "    vol_size=in_shape,\n",
    "    enc_nf=enc_nf,\n",
    "    dec_nf=dec_nf,\n",
    "    add_nf=add_nf,\n",
    "    lr=1e-04,\n",
    "    reg_weights=reg_weights,\n",
    ")\n",
    "n_param = utils.torch_model_parameters(model.reg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 2500\n",
    "steps = train_data.size\n",
    "max_steps = max_epochs * steps\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=max_epochs,\n",
    "    max_steps=max_steps,\n",
    "    log_every_n_steps=steps\n",
    ")\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = (256,) * 2\n",
    "reg_model = networks.VxmAffineFeatureDetector(\n",
    "    in_shape=in_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomAffine\n",
    "def create_square_image(image_size, square_size):\n",
    "    # Create a black background\n",
    "    image = np.zeros((image_size, image_size), dtype=np.uint8)\n",
    "\n",
    "    # Set pixels in the square to 255\n",
    "    start_x = (image_size - square_size) // 2\n",
    "    start_y = (image_size - square_size) // 2\n",
    "\n",
    "    image[start_x:start_x + square_size, start_y:start_y + square_size] = 255\n",
    "\n",
    "    return image\n",
    "\n",
    "aff_transformer = RandomAffine(degrees=0, translate=(0.20, 0.20))\n",
    "moving = torch.as_tensor(create_square_image(256, 100), device=device).unsqueeze(0).unsqueeze(1)\n",
    "moving = dm.minmax_norm(moving)\n",
    "fixed = aff_transformer(moving)\n",
    "out = reg_model(moving, fixed)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fixed.squeeze().tolist(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints(coords):\n",
    "    coords = np.asarray(coords)\n",
    "    x_coords = coords[:, 0]\n",
    "    y_coords = coords[:, 1]\n",
    "    plt.scatter(x_coords, y_coords, marker='o', color='r')\n",
    "\n",
    "cen_source, cen_target = out\n",
    "plot_keypoints(cen_source.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keypoints(cen_target.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthmorph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
