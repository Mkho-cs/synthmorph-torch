{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6kExhij7_gs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neurite as ne\n",
    "import voxelmorph as vxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent TF model from taking whole GPU memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reproducing the Demo (TF)\n",
    "Code in this section is exactly similar to\n",
    "[authors' demo](https://colab.research.google.com/drive/1GjpjkhKGrg5W-cvZVObBo3IoIUwaPZBU?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "iLR56ZWvxwjg",
    "outputId": "be6f49f8-a333-46dd-ccdf-bf788cac7e68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input shapes.\n",
    "in_shape = (256,) * 2\n",
    "num_dim = len(in_shape)\n",
    "num_label = 4\n",
    "num_maps = 40\n",
    "\n",
    "# Shape generation.\n",
    "label_maps = []\n",
    "for _ in tqdm.tqdm(range(num_maps)):\n",
    "    # Draw image and warp.\n",
    "    im = ne.utils.augment.draw_perlin(\n",
    "        out_shape=(*in_shape, num_label),\n",
    "        scales=(32, 64), max_std=1,\n",
    "    )\n",
    "    warp = ne.utils.augment.draw_perlin(\n",
    "        out_shape=(*in_shape, num_label, num_dim),\n",
    "        scales=(16, 32, 64), max_std=16,\n",
    "    )\n",
    "\n",
    "    # Transform and create label map.\n",
    "    im = vxm.utils.transform(im, warp)\n",
    "    lab = tf.argmax(im, axis=-1)\n",
    "    label_maps.append(np.uint8(lab))\n",
    "\n",
    "\n",
    "# Visualize shapes.\n",
    "num_row = 2\n",
    "per_row = 4\n",
    "for i in range(0, num_row * per_row, per_row):\n",
    "    ne.plot.slices(label_maps[i:i + per_row], cmaps=['tab20c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label_maps[0], cmap='tab20c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "vIP0i-sZ6ge8",
    "outputId": "e070f594-c34f-49b2-bcb8-5d635f891f7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image generation. For accurate registration, the landscape of generated warps\n",
    "# and image contrasts will need to include the target distribution.\n",
    "gen_arg = dict(\n",
    "    in_shape=in_shape,\n",
    "    in_label_list=np.unique(label_maps),\n",
    "    warp_std=3,\n",
    "    warp_res=(8, 16, 32),\n",
    ")\n",
    "gen_model_1 = ne.models.labels_to_image(**gen_arg, id=1)\n",
    "gen_model_2 = ne.models.labels_to_image(**gen_arg, id=2)\n",
    "\n",
    "\n",
    "# Test repeatedly for single input.\n",
    "num_gen = 8\n",
    "input = np.expand_dims(label_maps[0], axis=(0, -1))\n",
    "slices = [gen_model_1.predict(input)[0] for _ in range(num_gen)]\n",
    "ne.plot.slices(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8PIMBl0Idsk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Registration model.\n",
    "reg_model = vxm.networks.VxmDense(\n",
    "    inshape=in_shape,\n",
    "    int_resolution=2,\n",
    "    svf_resolution=2,\n",
    "    nb_unet_features=([256] * 4, [256] * 8),\n",
    "    reg_field='warp',\n",
    ")\n",
    "\n",
    "\n",
    "# Model for optimization.\n",
    "ima_1, map_1 = gen_model_1.outputs\n",
    "ima_2, map_2 = gen_model_2.outputs\n",
    "\n",
    "_, warp = reg_model((ima_1, ima_2))\n",
    "pred = vxm.layers.SpatialTransformer(fill_value=0)((map_1, warp))\n",
    "\n",
    "inputs = gen_model_1.inputs + gen_model_2.inputs\n",
    "out = (map_2, pred)\n",
    "model = tf.keras.Model(inputs, out)\n",
    "\n",
    "\n",
    "# Compilation.\n",
    "model.add_loss(vxm.losses.Dice().loss(*out) + tf.repeat(1., tf.shape(pred)[0]))\n",
    "model.add_loss(vxm.losses.Grad('l2', loss_mult=1).loss(None, warp))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "uCVXMonGyQrw",
    "outputId": "f272f582-934f-44a6-a781-bed4b9c730b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model for a few epochs. Re-running the cell will continue training.\n",
    "gen = vxm.generators.synthmorph(\n",
    "    label_maps,\n",
    "    batch_size=1,\n",
    "    same_subj=True,\n",
    "    flip=True,\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    gen,\n",
    "    initial_epoch=0,\n",
    "    epochs=4000,\n",
    "    steps_per_epoch=40,\n",
    "    verbose=1,\n",
    ") \n",
    "\n",
    "weights_dir = Path(\".\") / \"weights\" / \"keras\"\n",
    "model.save_weights(weights_dir / \"400k_original.h5\")\n",
    "\n",
    "# Visualize loss.\n",
    "plt.plot(hist.epoch, hist.history['loss'], '.-');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyoUu_BlexJs",
    "outputId": "12e5bbce-244f-4ab6-817c-b2ec219f0fba"
   },
   "outputs": [],
   "source": [
    "# Download model weights to skip training and save time.\n",
    "# !gdown -O weights.h5 1xridvtyEWgWsWJPYVrQfDCtSgbj2beRz\n",
    "# model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spmfHHiqWEoM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conform test data.\n",
    "def conform(x, in_shape=in_shape):\n",
    "    '''Resize and normalize image.'''\n",
    "    x = np.float32(x)\n",
    "    x = np.squeeze(x)\n",
    "    x = ne.utils.minmax_norm(x)\n",
    "    x = ne.utils.zoom(x, zoom_factor=[o / i for o, i in zip(in_shape, x.shape)])\n",
    "    return np.expand_dims(x, axis=(0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "BiBgAT5vyR54",
    "outputId": "b224c474-6d41-4a3c-c9be-9f4571483bb7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test on MNIST.\n",
    "images, digits = tf.keras.datasets.mnist.load_data()[-1]\n",
    "ind = np.flatnonzero(digits == 6)\n",
    "moving = conform(images[ind[233]])\n",
    "fixed = conform(images[ind[199]])\n",
    "moved, warp = reg_model.predict((moving, fixed))\n",
    "\n",
    "# Plot registration\n",
    "ne.plot.slices(\n",
    "    slices_in=(moving, fixed, moved),\n",
    "    titles=('Moving', 'Fixed', 'Moved'),\n",
    "    do_colorbars=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot warp matrix\n",
    "ne.plot.slices(\n",
    "    slices_in=(warp[..., 0], warp[..., 1]),\n",
    "    titles=('Warp (x-axis)', 'Warp (y-axis)'),\n",
    "    do_colorbars=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "vkPy-XQNEJp8",
    "outputId": "0f00b9c9-ea30-4880-98d8-6cc47f959ec3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test on OASIS-1.\n",
    "images = ne.py.data.load_dataset('2D-OASIS-TUTORIAL')\n",
    "moving = conform(images[2])\n",
    "fixed = conform(images[7])\n",
    "moved, warp = reg_model.predict((moving, fixed))\n",
    "\n",
    "\n",
    "ne.plot.slices(\n",
    "    slices_in=(moving, fixed, moved),\n",
    "    titles=('Moving', 'Fixed', 'Moved'),\n",
    "    do_colorbars=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Weights TF -> Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import neurite as ne\n",
    "import voxelmorph as vxm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vte.experiments.voxel_morph.model.synthmorph as models\n",
    "import vte.experiments.voxel_morph.model.synthmorph_new as new\n",
    "import vte.experiments.voxel_morph.datamodule.synth as datamodule\n",
    "import vte.experiments.voxel_morph.utils as utils\n",
    "from vte.experiments.voxel_morph.synthmorph_utils import(\n",
    "    conform as torch_conform, post_predict, image_to_numpy,\\\n",
    "    invert_grayscale, overlay_images,\\\n",
    "    plot_array_row, superimpose_circles,\\\n",
    "    convert_to_single_rgb, rotate\n",
    ")\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from cv2 import resize\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import pytorch_lightning as pl\n",
    "import torchinfo\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "import kornia.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fresh Torch model\n",
    "vol_size = (256,) * 2\n",
    "unet_enc_nf = [256] * 4\n",
    "unet_dec_nf = [256] * 8\n",
    "int_steps = 7 \n",
    "int_downsize = 2\n",
    "bidir = False\n",
    "torch_vxmdense = new.VxmDense(\n",
    "    inshape=vol_size,\n",
    "    nb_unet_features=[unet_enc_nf, unet_dec_nf],\n",
    "    int_steps=int_steps,\n",
    "    int_downsize=int_downsize,\n",
    "    bidir=bidir,\n",
    "    unet_half_res=True,\n",
    ")\n",
    "\n",
    "# torch_vxmdense.load_state_dict(\n",
    "#     torch.load('/home/jovyan/vte/vte/experiments/voxel_morph/model/torch_original.pth')\n",
    "# )\n",
    "\n",
    "torch_weights = torch_vxmdense.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define fresh Keras model\n",
    "# This section is just a copy of the demo to define the Keras model\n",
    "\n",
    "# Label maps\n",
    "in_shape = (256,) * 2\n",
    "num_dim = len(in_shape)\n",
    "num_label = 16\n",
    "num_maps = 1\n",
    "label_maps = []\n",
    "for _ in range(num_maps):\n",
    "    # Draw image and warp.\n",
    "    im = ne.utils.augment.draw_perlin(\n",
    "        out_shape=(*in_shape, num_label),\n",
    "        scales=(32, 64), max_std=1,\n",
    "    )\n",
    "    warp = ne.utils.augment.draw_perlin(\n",
    "        out_shape=(*in_shape, num_label, num_dim),\n",
    "        scales=(16, 32, 64), max_std=16,\n",
    "    )\n",
    "\n",
    "    # Transform and create label map.\n",
    "    im = vxm.utils.transform(im, warp)\n",
    "    lab = tf.argmax(im, axis=-1)\n",
    "    label_maps.append(np.uint8(lab))\n",
    "\n",
    "# Image generator\n",
    "gen_arg = dict(\n",
    "    in_shape=in_shape,\n",
    "    in_label_list=np.unique(label_maps),\n",
    "    warp_std=3,\n",
    "    warp_res=(8, 16, 32),\n",
    ")\n",
    "gen_model_1 = ne.models.labels_to_image(**gen_arg, id=1)\n",
    "gen_model_2 = ne.models.labels_to_image(**gen_arg, id=2)\n",
    "\n",
    "# Registration model.\n",
    "reg_model = vxm.networks.VxmDense(\n",
    "    inshape=in_shape,\n",
    "    int_resolution=2,\n",
    "    svf_resolution=2,\n",
    "    nb_unet_features=([256] * 4, [256] * 8),\n",
    "    reg_field='warp',\n",
    ")\n",
    "\n",
    "# Model for optimization.\n",
    "ima_1, map_1 = gen_model_1.outputs\n",
    "ima_2, map_2 = gen_model_2.outputs\n",
    "\n",
    "_, warp = reg_model((ima_1, ima_2))\n",
    "pred = vxm.layers.SpatialTransformer(fill_value=0)((map_1, warp))\n",
    "\n",
    "inputs = gen_model_1.inputs + gen_model_2.inputs\n",
    "out = (map_2, pred)\n",
    "model = tf.keras.Model(inputs, out)\n",
    "\n",
    "# Compilation.\n",
    "model.add_loss(vxm.losses.Dice().loss(*out) + tf.repeat(1., tf.shape(pred)[0]))\n",
    "model.add_loss(vxm.losses.Grad('l2', loss_mult=1).loss(None, warp))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Keras pretrained weights\n",
    "# Alternatively, download weights from https://drive.google.com/uc?id=1xridvtyEWgWsWJPYVrQfDCtSgbj2beRz\n",
    "# !gdown -O weights.h5 1xridvtyEWgWsWJPYVrQfDCtSgbj2beRz\n",
    "model.load_weights('weights.h5')\n",
    "# Extract weights from the registration model only\n",
    "keras_vxmdense = reg_model   \n",
    "keras_weights = {w.name: (w.numpy(), w.dtype, w.shape) for w in keras_vxmdense.weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only get kernel weights\n",
    "keras_weights_keys = list(keras_weights.keys())\n",
    "keras_kernels = [string for string in keras_weights_keys if 'bias' not in string]\n",
    "keras_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = {}\n",
    "\n",
    "# Transfer the weights (the order of layers are the same)\n",
    "for k,t in zip(keras_weights.keys(), torch_weights.keys()):\n",
    "    if k in keras_kernels:\n",
    "        new_weights[t] = torch.Tensor(np.moveaxis(keras_weights[k][0], [-1, -2], [0, 1]))\n",
    "    else:\n",
    "        new_weights[t] = torch.Tensor(keras_weights[k][0])\n",
    "\n",
    "torch_vxmdense.load_state_dict(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(torch_vxmdense.state_dict(), 'authors.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight transfer and Torch layers debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_keras_layers = keras_vxmdense.layers[:]\n",
    "custom_keras_model= Model(\n",
    "    inputs=keras_vxmdense.inputs, \n",
    "    outputs=custom_keras_layers[-1].output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conform test data.\n",
    "def conform(x, in_shape=in_shape):\n",
    "    '''Resize and normalize image.'''\n",
    "    x = np.float32(x)\n",
    "    x = np.squeeze(x)\n",
    "    x = ne.utils.minmax_norm(x)\n",
    "    x = ne.utils.zoom(x, zoom_factor=[o / i for o, i in zip(in_shape, x.shape)])\n",
    "    return np.expand_dims(x, axis=(0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, digits = tf.keras.datasets.mnist.load_data()[-1]\n",
    "ind = np.flatnonzero(digits == 5)\n",
    "moving = conform(images[ind[223]])\n",
    "fixed = conform(images[ind[199]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_torch = torch_vxmdense.unet_model\n",
    "flow_torch = torch_vxmdense.flow\n",
    "vecint_torch = torch_vxmdense.integrate\n",
    "rescale_torch = torch_vxmdense.fullsize\n",
    "spatial_torch = torch_vxmdense.transformer\n",
    "custom_torch_model = nn.Sequential(\n",
    "    unet_torch, \n",
    "    flow_torch, \n",
    "    vecint_torch,\n",
    "    rescale_torch,\n",
    "    spatial_torch,\n",
    ")\n",
    "custom_torch_model = custom_torch_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_torch(x):\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x.to('cuda')\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_moving = torch_conform(x=moving, in_shape=(256,256))\n",
    "torch_moving = preprocess_torch(torch_moving)\n",
    "torch_fixed = torch_conform(x=fixed, in_shape=(256,256))\n",
    "torch_fixed = preprocess_torch(torch_fixed)\n",
    "\n",
    "torch_unet_input = torch.cat([torch_moving, torch_fixed], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SSIM of TF vs Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssim_win_size = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keras_output = custom_keras_model.predict((moving, fixed))\n",
    "keras_output = keras_vxmdense.predict((moving, fixed))\n",
    "keras_source, keras_flow = keras_output\n",
    "\n",
    "keras_flow = keras_flow.transpose(0, 3, 1, 2)\n",
    "keras_source = keras_source.transpose(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch_output = custom_torch_model.forward(torch_unet_input)\n",
    "torch_output = torch_vxmdense(torch_moving, torch_fixed)['y_source']\n",
    "ssim_torch = torch.mean(metrics.ssim(torch_output, torch.tensor(keras_source).to('cuda'), ssim_win_size))\n",
    "print(f'ssim_torch = {ssim_torch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_output = keras_source.squeeze()\n",
    "# torch_output = custom_torch_model.forward(torch_unet_input)\n",
    "torch_output = torch_vxmdense(torch_moving, torch_fixed)['y_source']\n",
    "torch_output = torch_output.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "torch_output = torch_output.squeeze()\n",
    "print(keras_output.shape, torch_output.shape)\n",
    "ssim_skimage = structural_similarity(torch_output, keras_output, multichannel=True)\n",
    "ssim_skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF -> Torch functions debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voxelmorph interpn() function (Solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vol = np.random.randn(128, 128, 2)\n",
    "loc = np.random.randn(128, 128, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_vol = tf.constant(vol)\n",
    "keras_loc = tf.constant(loc)\n",
    "keras_interp = ne.utils.interpn(keras_vol, keras_loc, 'linear', None)\n",
    "keras_interp = tf.expand_dims(keras_interp, 0)\n",
    "keras_interp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_vol = torch.from_numpy(vol)\n",
    "torch_loc = torch.from_numpy(loc)\n",
    "torch_interp = utils.interpn(torch_vol, torch_loc, 'linear', None)\n",
    "torch_interp = torch.unsqueeze(torch_interp, 0)\n",
    "torch_interp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(torch_interp.numpy() == keras_interp.numpy()).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_interp = torch_interp.permute(0, 3, 1, 2)\n",
    "keras_interp = torch.tensor(keras_interp.numpy()).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interp_ssim = torch.mean(metrics.ssim(torch_interp, keras_interp, ssim_win_size))\n",
    "interp_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
